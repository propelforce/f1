{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753e8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "from sklearn.model_selection import TimeSeriesSplit,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5edd880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv(r'C:\\Users\\eswar\\f1\\data\\interim\\base_dataset_features.csv')\n",
    "# Target variable\n",
    "df_features['Top_3_finish'] = df_features['final_position'].le(3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76185bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = list(df_features.columns)\n",
    "drop_columns = ['race_date', 'qualifyId', 'wins', 'driver_race_points', 'constructor_race_points', 'driver_season_position', 'constructor_season_points', 'final_position', 'final_status']\n",
    "\n",
    "def difference(a, b):\n",
    "  \"\"\"Return the difference of two lists.\"\"\"\n",
    "  result = []\n",
    "  for item in a:\n",
    "    if item not in b:\n",
    "      result.append(item)\n",
    "  return result\n",
    "\n",
    "final_columns = difference(column_list, drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f5de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_features[df_features['year'] >= 2013].reset_index(drop=True)\n",
    "X = X[final_columns]\n",
    "y = X.Top_3_finish\n",
    "X = X.drop(['Top_3_finish'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebadff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #GridSearch\n",
    "\n",
    "# # Initialize a LightGBM classifier\n",
    "# clf = lgb.LGBMClassifier()\n",
    "\n",
    "# # Parameters to search\n",
    "# param_grid = {\n",
    "#     'num_leaves': [20, 31, 40],\n",
    "#     'learning_rate': [0.05, 0.1, 0.2],\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'objective': ['binary'],  # Specify binary classification objective\n",
    "#     'metric': ['binary_logloss'],  # Specify binary logloss as the metric\n",
    "#     'boosting_type': ['gbdt']  # Specify boosting type as gbdt\n",
    "# }\n",
    "\n",
    "# # Initialize a time series cross-validator\n",
    "# tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# # Use 'precision_weighted' and 'recall_weighted' as scoring metrics\n",
    "# grid_search = GridSearchCV(clf, param_grid, scoring=['precision_weighted', 'recall_weighted'], refit='precision_weighted', cv=tscv, verbose=2)\n",
    "\n",
    "# # Perform GridSearchCV\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "# # Get the best parameters\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# # Train the classifier with the best parameters\n",
    "# best_clf = lgb.LGBMClassifier(**best_params)\n",
    "# best_clf.fit(X, y)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = best_clf.predict(X)\n",
    "\n",
    "# # Compute metrics on the full dataset\n",
    "# conf_mat = confusion_matrix(y, y_pred)\n",
    "# precision = precision_score(y, y_pred, average='micro')\n",
    "# recall = recall_score(y, y_pred, average='micro')\n",
    "# f1 = f1_score(y, y_pred, average='weighted')\n",
    "\n",
    "# print(f\"Confusion Matrix:\\n{conf_mat}\")\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "# print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e93b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test split\n",
    "train_data = df_features[(df_features['year'] >= 2013) & (df_features['race_date'] < '2023-04-02')].reset_index(drop=True)\n",
    "test_data = df_features[(df_features['year'] == 2023) & (df_features['race_date'] == '2023-04-02')].reset_index(drop=True)\n",
    "\n",
    "# train_data = df_features[(df_features['year'] >= 2016) & (df_features['year'] < 2023)].reset_index(drop=True)\n",
    "# test_data = df_features[(df_features['year'] == 2023)].reset_index(drop=True)\n",
    "\n",
    "train_data = train_data[final_columns]\n",
    "test_data = test_data[final_columns]\n",
    "\n",
    "# Features and target variable\n",
    "X_train = train_data.drop(['Top_3_finish'], axis=1)\n",
    "y_train = train_data['Top_3_finish']\n",
    "\n",
    "X_test = test_data.drop(['Top_3_finish'], axis=1)\n",
    "y_test = test_data['Top_3_finish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcc66159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eswar\\anaconda3\\Lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 609, number of negative: 3530\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17580\n",
      "[LightGBM] [Info] Number of data points in the train set: 4139, number of used features: 226\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.147137 -> initscore=-1.757235\n",
      "[LightGBM] [Info] Start training from score -1.757235\n",
      "Feature Importance:\n",
      "                       Feature   Importance\n",
      "8          qualifying_position  8030.206424\n",
      "37          qualify_pos_minus4   339.475676\n",
      "11            final_pos_minus1   229.570832\n",
      "35    constructor_race_points3   171.128201\n",
      "206  constructor_race_points22   154.619291\n",
      "..                         ...          ...\n",
      "115            constructorId12     0.000000\n",
      "112                     wins12     0.000000\n",
      "79              constructorId8     0.000000\n",
      "103                     wins11     0.000000\n",
      "196            constructorId21     0.000000\n",
      "\n",
      "[226 rows x 2 columns]\n",
      "Accuracy: 0.85\n",
      "Precision: 0.5\n",
      "Recall: 0.3333333333333333\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91        17\n",
      "           1       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.85        20\n",
      "   macro avg       0.69      0.64      0.66        20\n",
      "weighted avg       0.83      0.85      0.84        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 20,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators' : 100\n",
    "}\n",
    "\n",
    "# Create LightGBM datasets\n",
    "train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "test_dataset = lgb.Dataset(X_test, label=y_test, reference=train_dataset)\n",
    "\n",
    "# Train the LightGBM model\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_dataset, num_round, valid_sets=[test_dataset])\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = bst.feature_importance(importance_type='gain')\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display feature importance\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_proba = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "y_pred_binary = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_binary))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_binary))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_binary))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57aa910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['pred_result'] = y_pred_binary\n",
    "test_data['Top_3_finish'] = y_test\n",
    "\n",
    "train_data['Top_3_finish'] = y_train\n",
    "\n",
    "train_data.to_csv(r'C:\\Users\\eswar\\f1\\data\\final\\train_data.csv',index=False)\n",
    "test_data.to_csv(r'C:\\Users\\eswar\\f1\\data\\final\\test_data.csv',index=False)\n",
    "test_data[['driverId','circuitId','raceId','year','race_month','race_day','Top_3_finish','pred_result']].to_csv(r'C:\\Users\\eswar\\f1\\data\\final\\f1-2023-preds-and-result.csv',index=False)\n",
    "feature_importance_df.to_csv(r'C:\\Users\\eswar\\f1\\data\\final\\feature_importance_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84b31ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\eswar\\f1\\models\\f1-race-top3-predictor.pkl', 'wb') as file:\n",
    "    pickle.dump(bst, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
